---
id: hugging-face-deployment
title: Running Tangle on Hugging Face
---

import { ImageAnnotation } from "@site/src/components/ImageAnnotation";
import { HelpCircle, Info, Delete, Maximize2, PlusSquare, Copy, ListRestart } from "lucide-react";

# Running Tangle on Hugging Face

This guide covers deploying and running TangleML on Hugging Face Spaces, from accessing the public playground to setting up your own team instance.

## Accessing Tangle on Hugging Face

There are two primary ways to access the Tangle application on Hugging Face:

### On the Hugging Face website

Navigate to the TangleML organization on Hugging Face at [huggingface.co/spaces/tangleml/tangle](https://huggingface.co/spaces/tangleml/tangle). Here you'll find a Tangle space in an embedded iframe where you can click to start using the application.

### Embedded iframe

There's also an embedded version at [tangleml-tangle.hf.space](https://tangleml-tangle.hf.space/). This version allows you to share run URLs, and maximizes vertical screen real estate.

Here's an example of embedding the iframe:

```html
<iframe
	src="https://tangleml-tangle.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>
```

## Multi-tenant architecture

The main Tangle instance on Hugging Face operates as a **multi-tenant system**, where:

- Each user works in complete isolation
- Every user has their own database for runs, components, and metadata
- Each user has separate data artifact storage
- Users cannot see or access other users' work

### Data privacy and storage

When using the multi-tenant Tangle:

- **Output artifacts** are stored in your personal Hugging Face dataset repository (at `your-username/tangle-data`)
- These repositories are **private by default**
- You maintain full ownership of your artifacts
- You can optionally make your data public through repository settings

<img src={require("./assets/HF_2.png").default} alt="Data Privacy and Storage" style={{width: "100%", borderRadius: "6px"}} />

:::warning
The run database containing metadata is currently stored in Tangle's persistent storage, not in your personal repository. This may change in future updates.
:::

### Where executions run

Pipeline executions run as Hugging Face jobs in your own account:

- Jobs run under your username, not under tangleml/tangle
- You can view and monitor job execution directly in Hugging Face
- Each execution links to its corresponding Hugging Face job

## Requirements and costs

### What you need

To use the shared Tangle instance on Hugging Face:

1. **Hugging Face account**: Create and log in to your account
2. **Permissions**: Grant Tangle access to:
   - Create repositories
   - Write to repositories  
   - Create jobs
3. **[Hugging Face Pro subscription](https://huggingface.co/pricing)**: Required for job execution

### Cost breakdown

:::tip
**Free to try**: You can explore the interface and create pipelines without a subscription. You only need Pro status to actually run jobs.
:::

- **Pro Subscription**: $9/month (required for job execution)
- **CPU Jobs**: Very affordable, almost negligible cost
- **GPU Jobs**: More expensive, depending on hardware and duration
- **Storage**: Your artifacts use your Hugging Face storage quota

## Creating your own Tangle instance

Teams or individuals who want their own dedicated Tangle instance can duplicate the space.

### How to duplicate

1. Navigate to the tangleml/tangle space
2. Click the three-dots menu
3. Select **Duplicate Space**

<img src={require("./assets/HF_4.png").default} alt="Duplicate Space" style={{width: "100%", borderRadius: "6px"}} />

### Configuration options

When duplicating, you'll need to configure:

| Setting | Instructions |
|---------|-------------|
| **Owner** | Choose your user account or organization |
| **Space name** | Name your Tangle instance |
| **Visibility** | <ul><li>Private (default): Only invited users can access</li><li>Public: Anyone can view runs (read-only)</li></ul> |
| **Hardware** | <ul><li>CPU Basic is sufficient for most users</li><li>No GPU required for the space itself</li></ul> |
| **Persistent Storage** | <ul><li>Minimum 20GB recommended</li><li>Required to preserve runs and components</li><li>**⚠️ Warning:** Avoid ephemeral mode. Without persistent storage, you'll lose all data when the space restarts.</li></ul> |
| **Hugging Face Token** | Create a token with permissions for:<ul><li>Repository management (**manage repos** or **contribute repos**)</li><li>Job submission (**jobs** permission)</li></ul> |


### Single-tenant vs multi-tenant differences

Your duplicated space operates differently from the shared instance:

#### Authentication
- Uses the configured token instead of individual user tokens
- Allows fine-grained permission control
- Can access private repositories if token has permissions

#### Multi-user support
- Multiple team members can use the same instance
- **Initiated by** field shows different users
- All users share the same runs database

#### Permissions model
User permissions in your Tangle instance mirror their organization roles:
- **Read-only** org members → Read-only in Tangle
- **Write** access → Can submit runs
- **Admin** → Full Tangle admin capabilities

:::tip
If you duplicate to a personal account (not an organization), you'll automatically be the admin of your instance.
:::

#### Data storage
- Artifacts stored in `your-space-name_data` repository
- All team members share the same artifact storage
- Database remains in the space's persistent storage

## Subscription requirements by setup

### Individual users
- **Shared Instance**: Pro subscription
- **Personal Instance**: Pro subscription + storage costs

### Teams and organizations
- **Organization Instance**: Team subscription
- **Required for**: Running jobs in organization namespace
- **Includes**: Collaboration features and shared resources

## Limitations on Hugging Face

### Storage constraints

The primary limitation is data storage:

- **Only dataset repositories** available for artifact storage
- No direct mounting of storage (unlike Kubernetes deployments)
- All data must be committed via Git operations
- Input/output requires explicit download/upload steps

:::warning
This adds overhead to pipeline execution as data must be downloaded before processing and uploaded after completion.
:::

### Container compatibility

Some technical requirements for containers:

- Must support Python installation (for Hugging Face CLI)
- Requires compatibility with uv package manager
- Issues with very old Alpine images (4-5 years old)
- Problems with musl-based containers vs glibc

:::tip
Most modern containers work without issues. Problems typically only occur with outdated or specialized minimal containers.
:::

### Component considerations

When creating Components for Hugging Face deployment:

**Data import/export**:
- Use Hugging Face-specific upload/download components
- Standard library components being developed for HF repositories
- Web downloads work universally

**Cross-cloud operations**:
- Authentication challenges when accessing other clouds (GCS, AWS)
- Requires credential management through private HF repositories
- Private repos can act as secret managers for credentials

**Cloud-specific services**:
- BigQuery, Vertex AI, etc. require special authentication
- Consider using Hugging Face native services (inference, serving)
- Plan for credential distribution in multi-cloud scenarios

## Public space access

Making your duplicated space public enables:

- Read-only access for non-authenticated users
- Public viewing of runs and logs
- Future: artifact previews and visualizations
- Shareable URLs for demonstrations

:::warning
Keep your space private if you're working with sensitive data. Public spaces allow anyone to view your pipeline runs.
:::

## Best practices

### For individuals
1. Start with the shared multi-tenant instance
2. Upgrade to Pro only when ready to run jobs
3. Monitor job costs, especially for GPU workloads

### For teams
1. Duplicate to your organization namespace
2. Configure appropriate persistent storage
3. Set up team permissions before inviting members
4. Consider data privacy requirements

### For component development
1. Test containers for Python/HF CLI compatibility
2. Plan data flow considering HF repository constraints
3. Handle credentials securely for cross-cloud operations
4. Leverage Hugging Face native services where possible